
idfRec{
  timeWeight = 0.1
  contentWeight = 0.4
  tagWeight = 0.3
  categoryWeight = 0.2
  modelPath = "/models/idfModel"
  numFeatures = 1048576
  candidateNewsPath = "/home/Jay/test/NLP/testdata/"
  thresHold = 0.001
  appName = "IDFModel"
  master = "local[4]"
}


# hdfs工作根目录
hdfsRoot="/apps/hduser1402/jk_news_proc"

# 训练数据路径
hdfTrainDataPath="/train_data/"

# 模型路径
hdfModelPath="/models/"

# 词典conf
hdfStopWordFile="/dicts/stopword/stopword.dic"    //停用词词典
hdfUserDictFile="/dicts/user_library/"             //切词人工词典
hdfFinTagFile="/dicts/others/financialTag"        //人工标签词典

#2^24
tfFeatureNum=33554432
idfMinDocFreq=2

# 文档id保存位置
lastTopicRecId="/records/last_topic_recid"
lastArticleRecId="/records/last_article_recid"
lastStockRecId="/records/last_stock_recid"

# hive结果存储位置
hiveCachePath="/hive_cache/"

topN = 10


hdfspath {
  # 项目根目录
  root = "/apps/hduser1402/pbear-sql"

  # hdfs上tasks目录，存放sql文件的目录，一个sql文件为一个任务,common用于存放公共任务脚本
  tasks = "/apps/hduser1402/pbear-sql/tasks/"
  tasksCommon = "/apps/hduser1402/pbear-sql/tasks/common/"

  # hdfs上存放jar目录，一般用于spark-submit --jars配置
  jars = "/apps/hduser1402/pbear-sql/jars/"
  jarsCommon = "/apps/hduser1402/pbear-sql/jars/common/"

  # hdfs上存放配置文件目录，一般用于spark-submit --files配置
  files = "/apps/hduser1402/pbear-sql/files/"
  filesCommon = "/apps/hduser1402/pbear-sql/files/common/"

  # hdfs上存放pbear-sql整个应用配置文件
  conf = "/apps/hduser1402/pbear-sql/conf/"

  # geoip数据库文件路径
  geoip = "/apps/hduser1402/pbear-sql/conf/GeoLite2-City.mmdb"

  # sparkstreming checkpoint 根目录，具体目录为：根目录+appname
  checkpoint = "/apps/hduser1402/pbear-sql/checkpoint/"

  # 运行结果存储目录, 再加上appname
  result = "/apps/hduser1402/pbear-sql/checkpoint/"
}

showDataframe {
  enable = true
  rows = 10
}
